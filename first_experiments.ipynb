{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from CIoTS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ps = list(range(2, 22, 2))\n",
    "runs = 20\n",
    "dimensions = 4\n",
    "data_length = 1000\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics = [\"aic\", \"bic\", \"hqic\", \"fpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_estimations = pd.DataFrame(columns=['p' ,'method', 'mean_p', 'std_p'])\n",
    "f1_scores = pd.DataFrame(columns=['p' ,'method', 'mean_f1', 'std_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in max_ps:\n",
    "    incoming_edges = max(ceil(dimensions*p/3), 1)\n",
    "    f1 = {ic: [] for ic in ics}\n",
    "    f1['real'] = []\n",
    "    \n",
    "    p_est = {ic: [] for ic in ics}\n",
    "    \n",
    "    for i in range(runs):\n",
    "        generator = CausalTSGenerator(dimensions=dimensions, max_p=p, data_length=data_length, incoming_edges=incoming_edges)\n",
    "        ts = generator.generate()\n",
    "        predicted_graph = pc_chen(partial_corr_test, ts, p, alpha)\n",
    "        \n",
    "        f1['real'].append(evaluate_edges(generator.graph, predicted_graph))\n",
    "        \n",
    "        var_ranking, var_scores = var_order_select(ts, 2*p, ics) \n",
    "        for ic in ics:\n",
    "            predicted_graph = pc_chen(partial_corr_test, ts, var_ranking[ic][0], alpha)\n",
    "        \n",
    "            p_est[ic].apped(var_ranking[ic][0])\n",
    "            f1[ic].append(evaluate_edges(generator.graph, predicted_graph))\n",
    "    \n",
    "    \n",
    "    f1_scores.append({'p': p, 'method': 'real', 'mean_f1': np.mean(f1['real']), 'std_f1': np.std(f1['real'])})\n",
    "    for ic in ics:\n",
    "        p_estimations.append({'p': p, 'method': ic, 'mean_p': np.mean(p_est[ic]), 'std_p': np.std(p_est[ic])})\n",
    "        f1_scores.append({'p': p, 'method': ic, 'mean_f1': np.mean(f1[ic]), 'std_f1': np.std(f1[ic])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.title('f1 scores')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('f1')\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "real, = plt.errorbar(x=f1_scores.loc[f1_scores['method']=='real', 'p'],\n",
    "                     y=f1_scores.loc[f1_scores['method']=='real', 'mean_f1'],\n",
    "                     yerr=f1_scores.loc[f1_scores['method']=='real', 'std_f1'],\n",
    "                     label='real')\n",
    "handles.append(real)\n",
    "labels.append('real')\n",
    "for ic in ics:\n",
    "    handle, label = plt.errorbar(x=f1_scores.loc[f1_scores['method']==ic, 'p'],\n",
    "                                 y=f1_scores.loc[f1_scores['method']==ic, 'mean_f1'],\n",
    "                                 yerr=f1_scores.loc[f1_scores['method']==ic, 'std_f1'],\n",
    "                                 label=ic)\n",
    "    handles.append(handle)\n",
    "    labels.append(label)\n",
    "plt.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.title('p estimations')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('p estimation')\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "real, = plt.errorbar(x=max_ps,\n",
    "                     y=max_ps,\n",
    "                     yerr=0,\n",
    "                     label='real')\n",
    "handles.append(real)\n",
    "labels.append('real')\n",
    "for ic in ics:\n",
    "    handle, label = plt.errorbar(x=p_estimations.loc[p_estimations['method']==ic, 'p'],\n",
    "                                 y=p_estimations.loc[p_estimations['method']==ic, 'mean_p'],\n",
    "                                 yerr=p_estimations.loc[p_estimations['method']==ic, 'std_p'],\n",
    "                                 label=ic)\n",
    "    handles.append(handle)\n",
    "    labels.append(label)\n",
    "plt.legend(handles, labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
