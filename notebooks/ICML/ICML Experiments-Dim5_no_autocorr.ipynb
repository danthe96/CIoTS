{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:02:33.040188Z",
     "start_time": "2019-01-16T12:02:32.989691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:02:33.086846Z",
     "start_time": "2019-01-16T12:02:33.044397Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.getcwd().endswith('CIoTS'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from CIoTS import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: check if data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:02:33.142755Z",
     "start_time": "2019-01-16T12:02:33.092937Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def check_setups(setups, data_path):\n",
    "    return not missing_setups(setups, data_path)\n",
    "\n",
    "def missing_setups(setups, data_path):\n",
    "    missing = []\n",
    "    for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "        if not os.path.isfile(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle'):\n",
    "            missing.append((dim, in_edges, tau, autocorr, run))\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T12:02:33.198573Z",
     "start_time": "2019-01-16T12:02:33.149291Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "dimensions = [5]\n",
    "incoming_edges = [2,3]\n",
    "taus = [5,10,15,20]\n",
    "autocorrs = [False]\n",
    "data_length = [10000]\n",
    "runs = range(10)\n",
    "\n",
    "\n",
    "setups = list(product(dimensions, incoming_edges, taus, autocorrs, data_length, runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute $F_1$ for known $\\tau$\n",
    "\n",
    "PC1 iterative vs. PC iterative\n",
    "\n",
    "Also check for $\\tau - k$ and $\\tau + k$ to visualize importance of $\\tau$ estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T14:06:55.826098Z",
     "start_time": "2019-01-16T12:02:33.206201Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_path = 'notebooks/ICML/icml_data_no_autocorr/'\n",
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "results = pd.DataFrame()\n",
    "k = 2\n",
    "\n",
    "algorithms = [(pc_incremental, 'PC incremental'),\n",
    "              (pc_incremental_extensive, 'PC extensive'),\n",
    "              (pc_incremental_pc1, 'PC1 incremental')]\n",
    "\n",
    "if not check_setups(setups, data_path):\n",
    "    print('Mising setups:')\n",
    "    print(missing_setups(setups, data_path))\n",
    "\n",
    "for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "    generator = pickle.load(open(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    \n",
    "    df_dict = {'dimension': dim, 'max time lag': tau, 'incoming edges': in_edges, 'run': run, 'autocorr': autocorr}\n",
    "    for algorithm, name in algorithms:\n",
    "        _, graphs, _, _, _ = algorithm(partial_corr_test, generator.ts, max_p=tau+k, \n",
    "                               use_stopper=False, alpha=0.01, verbose=True)\n",
    "        for offset in range(-k, k+1):\n",
    "            f1_score = evaluate_edges(generator.graph, graphs[tau+offset])['f1-score']\n",
    "            df_dict[name + '_f1' + (f'+{offset}' if offset>=0 else str(offset))] = f1_score\n",
    "\n",
    "    results = results.append(df_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "    results.to_csv(results_path + f'experiment1-Dim5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute estimate $\\hat{\\tau}$ for unknown $\\tau$\n",
    "\n",
    "Test VAR estimation vs. Incremental BIC (for $PC_1$) vs. Correltation test based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.517Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_path = 'notebooks/ICML/icml_data_no_autocorr/'\n",
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "results = pd.DataFrame()\n",
    "\n",
    "if not check_setups(setups, data_path):\n",
    "    print('Mising setups:')\n",
    "    print(missing_setups(setups, data_path))\n",
    "\n",
    "for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "    generator = pickle.load(open(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    \n",
    "    df_dict = {'dimension': dim, 'max time lag': tau, 'incoming edges': in_edges, 'run': run, 'autocorr': autocorr}\n",
    "    algorithms = [\n",
    "        (pc_incremental_pc1, {\n",
    "            'indep_test': partial_corr_test,\n",
    "            'ts': generator.ts,\n",
    "            'max_p': 2*tau,\n",
    "            'stopper': ICStopper(dim, patiency=2, ic='bic'),\n",
    "            'verbose': True}, lambda r: r[3].best_tau, 'PC1 incremental: BIC'),\n",
    "        (pc_incremental_pc1, {\n",
    "            'indep_test': partial_corr_test,\n",
    "            'ts': generator.ts,\n",
    "            'max_p': 2*tau,\n",
    "            'stopper': CorrStopper(dim, patiency=1, max_tau=2*tau),\n",
    "            'verbose': True}, lambda r: r[3].best_tau, 'PC1 incremental: Correlation'),\n",
    "        (var_order_select, {\n",
    "            'ts': generator.ts, \n",
    "            'max_p': 2*tau, \n",
    "            'ics': ['bic']}, lambda r: r[0]['bic'][0], 'VAR: BIC'),\n",
    "        (pc_incremental, {\n",
    "            'indep_test': partial_corr_test,\n",
    "            'ts': generator.ts,\n",
    "            'max_p': 2*tau,\n",
    "            'stopper': ICStopper(dim, patiency=2, ic='bic'),\n",
    "            'verbose': True}, lambda r: r[3].best_tau, 'PC incremental: BIC'),\n",
    "        (pc_incremental_extensive, {\n",
    "            'indep_test': partial_corr_test,\n",
    "            'ts': generator.ts,\n",
    "            'max_p': 2*tau,\n",
    "            'stopper': ICStopper(dim, patiency=2, ic='bic'),\n",
    "            'verbose': True}, lambda r: r[3].best_tau, 'PC extensive: BIC')\n",
    "    ]\n",
    "    for algorithm, params, result_func, name in algorithms:\n",
    "        result = algorithm(**params)\n",
    "        df_dict[name + '_tau'] = result_func(result)\n",
    "    results = results.append(df_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "    results.to_csv(results_path + f'experiment2-Dim5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.573Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_path = 'notebooks/ICML/icml_data_no_autocorr/'\n",
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "results = pd.DataFrame()\n",
    "\n",
    "algorithms = [(pc_incremental, 'PC incremental'),\n",
    "              (pc_incremental_extensive, 'PC extensive'),\n",
    "              (pc_incremental_pc1, 'PC1 incremental')]\n",
    "\n",
    "if not check_setups(setups, data_path):\n",
    "    print('Mising setups:')\n",
    "    print(missing_setups(setups, data_path))   \n",
    "    \n",
    "for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "    generator = pickle.load(open(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    max_tau = 2 * tau\n",
    "    \n",
    "    df_dict = {'dimension': [dim]*max_tau, 'max time lag': [tau]*max_tau, 'incoming edges': [in_edges]*max_tau, \n",
    "               'run': [run]*max_tau, 'autocorr': [autocorr]*max_tau, 'tau estimate': list(range(1, max_tau+1))}\n",
    "    \n",
    "    for algorithm, name in algorithms:\n",
    "        _, graphs, _, stopper, _ = algorithm(partial_corr_test, generator.ts, max_p=max_tau, \n",
    "                                             use_stopper=False, alpha=0.01, verbose=True)\n",
    "\n",
    "        confusion, confusion_delta = evaluate_edge_deletion(generator.graph,\n",
    "                                                            [{'graph': graphs[t], 'p_iter': t} \n",
    "                                                             for t in range(1, max_tau+1)],\n",
    "                                                            dim)\n",
    "        added_edges = (confusion_delta['tp'] + confusion_delta['fp']).tolist()\n",
    "        f1_scores = [evaluate_edges(generator.graph, graphs[t])['f1-score'] for t in range(1, max_tau+1)]\n",
    "        bics = [stopper.scores()[t] for t in range(1, max_tau+1)]\n",
    "        \n",
    "        eval_dict = {name + '_tn': confusion['tn'].tolist(), name + '_fp': confusion['fp'].tolist(),\n",
    "                     name + '_tp': confusion['tp'].tolist(), name + '_fn': confusion['fn'].tolist(),\n",
    "                     name + '_f1': f1_scores, name + '_bics': bics, name + '_added_edges': added_edges}\n",
    "        df_dict.update(eval_dict)\n",
    "        \n",
    "    df = pd.DataFrame(df_dict)\n",
    "    results = results.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "    results.to_csv(results_path + f'experiment3-Dim5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.575Z"
    }
   },
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualize $F_1$ for known $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.684Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import sys\n",
    "eps = sys.float_info.epsilon\n",
    "\n",
    "def plot_f1_groupedbarchart(df_dict, title):\n",
    "    assert len(df_dict) > 0\n",
    "    #assert reduce(lambda x,y: np.all(x == y), map(lambda e:e.columns, grouped_result.values()))\n",
    "    bar_labels = [f'$\\\\tau + {c[1]}$' if c[1]>=0 else f'$\\\\tau - {abs(c[1])}$'\n",
    "                  for c in next(iter(grouped_result.values())).columns]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    ind = np.arange(len(bar_labels))\n",
    "    width = 0.7 / len(df_dict)\n",
    "    \n",
    "    plots = []\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    for w_idx, (algo, f1s) in enumerate(df_dict.items()):\n",
    "        p = ax.bar(ind + w_idx*width + (width / 2), np.mean(f1s, axis=0), width, \n",
    "                   color=color_cycle[w_idx % len(color_cycle)],\n",
    "                   bottom=0, yerr=np.std(f1s, axis=0))\n",
    "        plots.append(p[0])\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(ind + (len(df_dict) * width) / 2)\n",
    "    ax.set_xticklabels(bar_labels)\n",
    "\n",
    "    ax.legend(plots, df_dict.keys())\n",
    "    ax.autoscale_view()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.688Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "algorithms = [(pc_incremental, 'PC incremental'),\n",
    "              (pc_incremental_extensive, 'PC extensive'),\n",
    "              (pc_incremental_pc1, 'PC1 incremental')]\n",
    "\n",
    "results = pd.read_csv(results_path + 'experiment1-Dim5.csv')\n",
    "for group, result in results.groupby(['max time lag', 'autocorr']): # Group by ['dimension', 'incoming edges', 'autocorr'] as well?\n",
    "    tau = int(group[0])\n",
    "    autocorr = float(group[1])\n",
    "    grouped_result = {}\n",
    "    for algorithm, name in algorithms:\n",
    "        algo_cols = [col for col in result.columns if col[:col.rfind('_')] == name]\n",
    "        df = result[algo_cols].rename(lambda c:('f1', int(c.split('_')[-1][2:])), axis='columns')\n",
    "        grouped_result[name] = df[sorted(df.columns)]\n",
    "    plot_f1_groupedbarchart(grouped_result, title=f'$F_1$ score for known $\\\\tau$ = {tau}, autocorr = {autocorr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize estimate $\\hat{\\tau}$ for unknown $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.740Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "visualize = [(3, 2, False), (3, 3, False)]\n",
    "names = ['PC1 incremental: BIC', 'PC1 incremental: Correlation',\n",
    "         'VAR: BIC', 'PC incremental: BIC', 'PC extensive: BIC']\n",
    "\n",
    "width = 0.4\n",
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "cols = [name + '_tau' for name in names]\n",
    "\n",
    "results = pd.read_csv(results_path + 'experiment2-Dim5.csv')\n",
    "true_taus = np.unique(results['max time lag'])\n",
    "\n",
    "for group, result in results.groupby(['dimension', 'incoming edges', 'autocorr']): \n",
    "    \n",
    "    if group not in visualize:\n",
    "        continue\n",
    "    \n",
    "    dim = int(group[0])\n",
    "    in_edges = int(group[1])\n",
    "    autocorr = float(group[2])\n",
    "    \n",
    "    colors = plt.cm.CMRmap(np.linspace(0,1,len(names)+2))\n",
    "    plt.figure(dpi=200, figsize=(15, 8))\n",
    "    \n",
    "    \n",
    "    for tau in true_taus:\n",
    "        start_x = tau - (width+0.1)*(len(names))/2\n",
    "        end_x = tau + (width+0.1)*(len(names))/2\n",
    "        y = tau\n",
    "        plt.hlines(y, start_x, end_x, colors='grey')\n",
    "    \n",
    "    bps = []\n",
    "    for i, name in enumerate(names):\n",
    "        positions = [tau + (width+0.1)*(i - (len(names)-1)/2) for tau in true_taus]\n",
    "        arr = [result.loc[result['max time lag']==tau, cols[i]] for tau in true_taus]\n",
    "        bp = plt.boxplot(arr, positions=positions, widths=width, showfliers=False,\n",
    "                         whiskerprops={'color': colors[i+1]}, boxprops={'color': colors[i+1]},\n",
    "                         capprops={'color': colors[i+1]}, medianprops={'color': 'black'})\n",
    "        bps.append(bp)\n",
    "    \n",
    "    plt.xlabel('true $\\\\tau$')\n",
    "    plt.ylabel('estimated $\\\\hat{\\\\tau}$')\n",
    "    plt.xticks(true_taus, true_taus)\n",
    "    plt.title(f'$\\\\tau$ estimation for dimensionality={dim}, incoming edges={in_edges}, autocorr={autocorr}')\n",
    "    plt.xlim(min(true_taus)-(width+0.2)*len(names)/2, max(true_taus)+(width+0.2)*len(names)/2)\n",
    "    \n",
    "    plt.legend([bps[i]['whiskers'][0] for i in range(len(names))], names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying patiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.793Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_stopping(df, stopping, params, col):\n",
    "    sorted_df = df.sort_values(by='tau estimate')\n",
    "    idx = stopping.simulate(sorted_df[col].values, **params)\n",
    "    return sorted_df['tau estimate'].iloc[idx]\n",
    "\n",
    "from math import floor\n",
    "\n",
    "visualize = [(3, 2, False), (3, 3, False),(3, 4, False)]\n",
    "\n",
    "# schema: pc_version, stopper_class, params, column_suffix, name\n",
    "algorithms = [('PC1 incremental', ICStopper, {'patiency': 2}, 'bics', 'PC1: BIC'),\n",
    "              ('PC extensive', ICStopper, {'patiency': 2}, 'bics', 'PC extensive: BIC'),\n",
    "              ('PC1 incremental', CorrStopper, {'patiency': 1}, 'added_edges', 'PC1: Correlation'),\n",
    "              ('PC extensive', CorrStopper, {'patiency': 1}, 'added_edges', 'PC extensive: Correlation')]\n",
    "\n",
    "width = 0.4\n",
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "\n",
    "names = [name for _, _, _, _, name in algorithms]\n",
    "cols = [name + '_tau' for name in names]\n",
    "\n",
    "results = pd.read_csv(results_path + 'experiment3-Dim5.csv')\n",
    "true_taus = np.unique(results['max time lag'])\n",
    "\n",
    "for group, result in results.groupby(['dimension', 'incoming edges', 'autocorr']): \n",
    "    \n",
    "    if group not in visualize:\n",
    "        continue\n",
    "    \n",
    "    dim = int(group[0])\n",
    "    in_edges = int(group[1])\n",
    "    autocorr = float(group[2])\n",
    "    \n",
    "    colors = plt.cm.CMRmap(np.linspace(0,1,len(names)+2))\n",
    "    plt.figure(dpi=200, figsize=(15, 8))\n",
    "    \n",
    "    for tau in true_taus:\n",
    "        start_x = tau - (width+0.1)*(len(names))/2\n",
    "        end_x = tau + (width+0.1)*(len(names))/2\n",
    "        y = tau\n",
    "        plt.hlines(y, start_x, end_x, colors='grey')\n",
    "    \n",
    "    bps = []\n",
    "    for i, (pc, stopper, params, col_suffix, name) in enumerate(algorithms):\n",
    "        col = pc + '_' + col_suffix\n",
    "        positions = [tau + (width+0.1)*(i - (len(names)-1)/2) for tau in true_taus]\n",
    "        arr = [result[result['max time lag'] == tau].groupby(['run']).apply(lambda df: simulate_stopping(df, \n",
    "                                                                                                         stopper, \n",
    "                                                                                                         params, \n",
    "                                                                                                         col)).values \n",
    "               for tau in true_taus]\n",
    "        bp = plt.boxplot(arr, positions=positions, widths=width, showfliers=False,\n",
    "                         whiskerprops={'color': colors[i+1]}, boxprops={'color': colors[i+1]},\n",
    "                         capprops={'color': colors[i+1]}, medianprops={'color': 'black'})\n",
    "        bps.append(bp)\n",
    "    \n",
    "    plt.xlabel('true $\\\\tau$')\n",
    "    plt.ylabel('estimated $\\\\hat{\\\\tau}$')\n",
    "    plt.xticks(true_taus, true_taus)\n",
    "    plt.title(f'$\\\\tau$ estimation for dimensionality={dim}, incoming edges={in_edges}, autocorr={autocorr}')\n",
    "    plt.xlim(min(true_taus)-(width+0.2)*len(names)/2, max(true_taus)+(width+0.2)*len(names)/2)\n",
    "    \n",
    "    plt.legend([bps[i]['whiskers'][0] for i in range(len(names))], names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-16T12:02:34.846Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "visualize = [(20, 3, 4, False)]\n",
    "names = ['PC1 incremental', 'PC extensive', 'PC incremental']\n",
    "prop = 'bics'\n",
    "\n",
    "width = 0.4\n",
    "results_path = 'notebooks/ICML/icml_results_no_autocorr/'\n",
    "\n",
    "results = pd.read_csv(results_path + 'experiment3-Dim5.csv')\n",
    "true_taus = np.unique(results['max time lag'])\n",
    "\n",
    "for group, result in results.groupby(['max time lag', 'dimension', 'incoming edges', 'autocorr', 'run']): \n",
    "    \n",
    "    tau = int(group[0])\n",
    "    dim = int(group[1])\n",
    "    in_edges = int(group[2])\n",
    "    autocorr = float(group[3])\n",
    "    run = int(group[4])\n",
    "    \n",
    "    if (tau, dim, in_edges, autocorr) not in visualize:\n",
    "        continue\n",
    "    \n",
    "    colors = plt.cm.CMRmap(np.linspace(0,1,len(names)+2))\n",
    "    plt.figure(dpi=200, figsize=(15, 8))\n",
    "    \n",
    "    x = result['tau estimate']\n",
    "    for i, name in enumerate(names):\n",
    "        y = result[name + '_' + prop]\n",
    "        plt.plot(x, y, color=colors[i+1], label=name)\n",
    "    \n",
    "    plt.xlabel('iteration $\\\\tau$')\n",
    "    plt.ylabel(prop)\n",
    "    plt.title(f'{prop} for dimensionality={dim}, incoming edges={in_edges}, autocorr={autocorr}, run={run}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
