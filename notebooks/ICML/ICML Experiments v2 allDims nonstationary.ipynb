{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T11:33:34.700361Z",
     "start_time": "2019-01-31T11:33:34.654751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T11:33:35.596204Z",
     "start_time": "2019-01-31T11:33:35.565413Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.getcwd().endswith('CIoTS'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from CIoTS import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: check if data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T11:33:38.220992Z",
     "start_time": "2019-01-31T11:33:38.174584Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def check_setups(setups, data_path):\n",
    "    return not missing_setups(setups, data_path)\n",
    "\n",
    "def missing_setups(setups, data_path):\n",
    "    missing = []\n",
    "    for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "        if not os.path.isfile(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle'):\n",
    "            missing.append((dim, in_edges, tau, autocorr, run))\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T11:33:40.445713Z",
     "start_time": "2019-01-31T11:33:40.399213Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "dimensions = [3,5,10]\n",
    "incoming_edges = [2,3,4]\n",
    "taus = [5,10,15,20]\n",
    "autocorrs = [False, True]\n",
    "data_length = [1000]\n",
    "runs = range(10)\n",
    "\n",
    "\n",
    "setups = list(product(dimensions, incoming_edges, taus, autocorrs, data_length, runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-29T21:25:17.038Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Daniel.Thevessen/Causality/CIoTS/src/tigramite/tigramite/independence_tests.py:1144: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  beta_hat = numpy.linalg.lstsq(z, y)[0]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "data_path = 'notebooks/ICML/nonstationary_data/'\n",
    "results_path = 'notebooks/ICML/icml_results_nonstationary/'\n",
    "results = pd.read_csv(results_path + f'experiment3.csv')\n",
    "\n",
    "algorithms = [(pc_incremental, 'PC incremental'),\n",
    "              (pc_incremental_extensive, 'PC extensive'),\n",
    "              (pc_incremental_pc1, 'PC1 incremental'),\n",
    "              (pc_incremental_pc1mci, 'PCMCI incremental')]\n",
    "\n",
    "if not check_setups(setups, data_path):\n",
    "    print('Mising setups:')\n",
    "    print(missing_setups(setups, data_path))   \n",
    "    print(len(missing_setups(setups, data_path)))\n",
    "    \n",
    "for dim, in_edges, tau, autocorr, _, run in setups[82:]:\n",
    "    generator = pickle.load(open(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    max_tau = 2 * tau\n",
    "    \n",
    "    df_dict = {'dimension': [dim]*max_tau, 'max time lag': [tau]*max_tau, 'incoming edges': [in_edges]*max_tau, \n",
    "               'run': [run]*max_tau, 'autocorr': [autocorr]*max_tau, 'tau estimate': list(range(1, max_tau+1))}\n",
    "    \n",
    "    algo_graphs = {}\n",
    "    for algorithm, name in algorithms:\n",
    "        _, graphs, _, stopper, _ = algorithm(tigramite_partial_corr_test, generator.ts, max_p=max_tau, \n",
    "                                             use_stopper=False, alpha=0.01, verbose=True)\n",
    "\n",
    "        algo_graphs[name] = graphs\n",
    "        confusion, confusion_delta = evaluate_edge_deletion(generator.graph,\n",
    "                                                            [{'graph': graphs[t], 'p_iter': t} \n",
    "                                                             for t in range(1, max_tau+1)],\n",
    "                                                            dim)\n",
    "        added_edges = (confusion_delta['tp'] + confusion_delta['fp']).tolist()\n",
    "        f1_scores = [evaluate_edges(generator.graph, graphs[t])['f1-score'] for t in range(1, max_tau+1)]\n",
    "        bics = [stopper.scores()[t] for t in range(1, max_tau+1)]\n",
    "        \n",
    "        eval_dict = {name + '_tn': confusion['tn'].tolist(), name + '_fp': confusion['fp'].tolist(),\n",
    "                     name + '_tp': confusion['tp'].tolist(), name + '_fn': confusion['fn'].tolist(),\n",
    "                     name + '_f1': f1_scores, name + '_bics': bics, name + '_added_edges': added_edges}\n",
    "        df_dict.update(eval_dict)\n",
    "        \n",
    "    pickle.dump(algo_graphs, open(results_path + 'exp3_graphs/' + \n",
    "                                  f'graphs-t={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'wb'))\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    results = results.append(df, ignore_index=True)\n",
    "\n",
    "    results.to_csv(results_path + f'experiment3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate VAR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T12:31:47.973062Z",
     "start_time": "2019-01-31T11:33:52.322313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Daniel.Thevessen/Causality/CIoTS/CIoTS/evaluation.py:123: RuntimeWarning: Mean of empty slice.\n",
      "  mse_fp = se[(true_params == 0) & (est_params != 0)].mean()\n",
      "/home/Daniel.Thevessen/.local/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def graph2Var(graph, dim, tau, mapping):\n",
    "    inverted_mapping = {v: k for k, v in mapping.items()}\n",
    "    params = np.zeros((dim * tau, dim))\n",
    "\n",
    "    for x_t in range(dim):\n",
    "        input_nodes = list(graph.predecessors(mapping[x_t]))\n",
    "        inputs = np.array([inverted_mapping[x] for x in input_nodes])\n",
    "        for i in inputs:\n",
    "            params[i - dim, x_t] = graph.edges[(mapping[i], mapping[x_t])]['weight']\n",
    "    return params\n",
    "\n",
    "data_path = 'notebooks/ICML/nonstationary_data/'\n",
    "results_path = 'notebooks/ICML/icml_results_nonstationary/'\n",
    "results = pd.DataFrame()\n",
    "\n",
    "names = ['PC1 incremental', 'PC extensive', 'PC incremental', 'PCMCI incremental']\n",
    "\n",
    "for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "    generator = pickle.load(open(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    algo_graphs = pickle.load(open(results_path + 'exp3_graphs/' + \n",
    "                                   f'graphs-t={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    max_tau = 2*tau\n",
    "    df_dict = {'dimension': [dim]*max_tau, 'max time lag': [tau]*max_tau, 'incoming edges': [in_edges]*max_tau, \n",
    "               'run': [run]*max_tau, 'autocorr': [autocorr]*max_tau, 'tau estimate': list(range(1, max_tau+1))}\n",
    "    \n",
    "    true_mapping, true_matrix = transform_ts(generator.ts, tau)\n",
    "    true_params = graph2Var(generator.graph, dim, tau, true_mapping)\n",
    "    \n",
    "    # each algorithm\n",
    "    for name in names:\n",
    "        df_dict[name] = []\n",
    "        graphs = algo_graphs[name]\n",
    "        \n",
    "        assert len(graphs) == max_tau\n",
    "        for est_tau, graph in graphs.items():\n",
    "            model = VAR(est_tau)\n",
    "            node_mapping, data_matrix = transform_ts(generator.ts, est_tau)\n",
    "            model.fit_from_graph(dim, data_matrix, graph, node_mapping)\n",
    "            est_params = model.params[1:]\n",
    "            df_dict[name].append(evaluate_parameters(true_params, est_params))\n",
    "    \n",
    "    # VAR for different tau'\n",
    "    df_dict['complete VAR'] = []\n",
    "    for est_tau in range(1, max_tau+1):\n",
    "        model = VAR(est_tau)\n",
    "        model.fit(generator.ts)\n",
    "        est_params = model.params[1:]\n",
    "        df_dict['complete VAR'].append(evaluate_parameters(true_params, est_params))\n",
    "    \n",
    "    # True Graph VAR\n",
    "    model = VAR(tau)\n",
    "    model.fit_from_graph(dim, true_matrix, generator.graph, true_mapping)\n",
    "    est_params = model.params[1:]\n",
    "    df_dict['true Graph'] = [evaluate_parameters(true_params, est_params)] * max_tau\n",
    "    \n",
    "    df = pd.DataFrame(df_dict)\n",
    "    results = results.append(df)\n",
    "    \n",
    "    results.to_csv(results_path + f'experiment4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Incremental vs Non-Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T18:32:05.853913Z",
     "start_time": "2019-01-31T12:31:48.250392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Daniel.Thevessen/Causality/CIoTS/src/tigramite/tigramite/independence_tests.py:1144: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  beta_hat = numpy.linalg.lstsq(z, y)[0]\n",
      "/home/Daniel.Thevessen/Causality/CIoTS/CIoTS/evaluation.py:123: RuntimeWarning: Mean of empty slice.\n",
      "  mse_fp = se[(true_params == 0) & (est_params != 0)].mean()\n",
      "/home/Daniel.Thevessen/.local/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/stats/stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def graph2Var(graph, dim, tau, mapping):\n",
    "    inverted_mapping = {v: k for k, v in mapping.items()}\n",
    "    params = np.zeros((dim * tau, dim))\n",
    "\n",
    "    for x_t in range(dim):\n",
    "        input_nodes = list(graph.predecessors(mapping[x_t]))\n",
    "        inputs = np.array([inverted_mapping[x] for x in input_nodes])\n",
    "        for i in inputs:\n",
    "            params[i - dim, x_t] = graph.edges[(mapping[i], mapping[x_t])]['weight']\n",
    "    return params\n",
    "\n",
    "data_path = 'notebooks/ICML/nonstationary_data/'\n",
    "results_path = 'notebooks/ICML/icml_results_nonstationary/'\n",
    "results = pd.DataFrame()\n",
    "\n",
    "k = 2\n",
    "\n",
    "if not check_setups(setups, data_path):\n",
    "    print('Mising setups:')\n",
    "    print(missing_setups(setups, data_path))\n",
    "\n",
    "for dim, in_edges, tau, autocorr, _, run in setups:\n",
    "    generator = pickle.load(open(data_path + f't={tau}_d={dim}_in={in_edges}_autocorr={autocorr}_{run}.pickle', 'rb'))\n",
    "    \n",
    "    true_mapping, true_matrix = transform_ts(generator.ts, tau)\n",
    "    true_params = graph2Var(generator.graph, dim, tau, true_mapping)\n",
    "    \n",
    "    df_dict = {'dimension': dim, 'max time lag': tau, 'incoming edges': in_edges, 'run': run, 'autocorr': autocorr}\n",
    "    \n",
    "    # incremental\n",
    "    stopper = ICStopper(dim=dim, patiency=2)\n",
    "    graph = pc_incremental_pc1(tigramite_partial_corr_test, ts=generator.ts, max_p=2*tau, stopper=stopper)\n",
    "    eval_results = evaluate_edges(generator.graph, graph)\n",
    "    for name, score in eval_results.items():\n",
    "        df_dict[f'PC1 incremental - {name}'] = score\n",
    "    mapping, matrix = transform_ts(generator.ts, stopper.best_tau)\n",
    "    model = VAR(stopper.best_tau)\n",
    "    model.fit_from_graph(dim, matrix, graph, mapping)\n",
    "    est_params = model.params[1:]\n",
    "    mses = evaluate_parameters(true_params, est_params)\n",
    "    df_dict.update({'PC1 incremental - MSE full': mses[0],\n",
    "                    'PC1 incremental - MSE TR': mses[1],\n",
    "                    'PC1 incremental - MSE FP': mses[2]})\n",
    "    \n",
    "    # non-incremental\n",
    "    for offset in range(-k, k+1):\n",
    "        offset_str = f'{offset:+}' if offset != 0 else ''\n",
    "        graph = pc_incremental_pc1(tigramite_partial_corr_test, ts=generator.ts, \n",
    "                                   start=0, step=tau+offset, max_p=tau+offset)\n",
    "        eval_results = evaluate_edges(generator.graph, graph)\n",
    "        for name, score in eval_results.items():\n",
    "            df_dict[f'PC1 tau{offset_str} - {name}'] = score\n",
    "        mapping, matrix = transform_ts(generator.ts, tau+offset)\n",
    "        model = VAR(tau+offset)\n",
    "        model.fit_from_graph(dim, matrix, graph, mapping)\n",
    "        est_params = model.params[1:]\n",
    "        mses = evaluate_parameters(true_params, est_params)\n",
    "        df_dict.update({f'PC1 tau{offset_str} - MSE full': mses[0],\n",
    "                        f'PC1 tau{offset_str} - MSE TR': mses[1],\n",
    "                        f'PC1 tau{offset_str} - MSE FP': mses[2]})\n",
    "    \n",
    "    results = results.append(df_dict, ignore_index=True)\n",
    "    results.to_csv(results_path + f'experiment5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-18T10:56:28.044Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "visualize = [(20, 3, 4, False)]\n",
    "names = ['PC1 incremental', 'PC extensive', 'PC incremental']\n",
    "prop = 'bics'\n",
    "\n",
    "width = 0.4\n",
    "results_path = 'notebooks/ICML/icml_results_v2/'\n",
    "\n",
    "results = pd.read_csv(results_path + 'experiment3.csv')\n",
    "true_taus = np.unique(results['max time lag'])\n",
    "\n",
    "for group, result in results.groupby(['max time lag', 'dimension', 'incoming edges', 'autocorr', 'run']): \n",
    "    \n",
    "    tau = int(group[0])\n",
    "    dim = int(group[1])\n",
    "    in_edges = int(group[2])\n",
    "    autocorr = float(group[3])\n",
    "    run = int(group[4])\n",
    "    \n",
    "    if (tau, dim, in_edges, autocorr) not in visualize:\n",
    "        continue\n",
    "    \n",
    "    colors = plt.cm.CMRmap(np.linspace(0,1,len(names)+2))\n",
    "    plt.figure(dpi=200, figsize=(15, 8))\n",
    "    \n",
    "    x = result['tau estimate']\n",
    "    for i, name in enumerate(names):\n",
    "        y = result[name + '_' + prop]\n",
    "        plt.plot(x, y, color=colors[i+1], label=name)\n",
    "    \n",
    "    plt.xlabel('iteration $\\\\tau$')\n",
    "    plt.ylabel(prop)\n",
    "    plt.title(f'{prop} for dimensionality={dim}, incoming edges={in_edges}, autocorr={autocorr}, run={run}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
