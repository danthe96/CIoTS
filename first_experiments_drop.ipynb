{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T11:59:10.969449Z",
     "start_time": "2018-06-18T11:59:10.952075Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T11:59:11.976911Z",
     "start_time": "2018-06-18T11:59:10.970881Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from CIoTS import *\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T11:59:12.004112Z",
     "start_time": "2018-06-18T11:59:11.978469Z"
    }
   },
   "outputs": [],
   "source": [
    "runs = 20\n",
    "max_p = 22\n",
    "dimensions = 3\n",
    "data_length = 10000\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T11:59:12.030464Z",
     "start_time": "2018-06-18T11:59:12.005666Z"
    }
   },
   "outputs": [],
   "source": [
    "ics = [\"aic\", \"bic\", \"hqic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T12:33:16.072889Z",
     "start_time": "2018-06-18T12:33:16.042249Z"
    }
   },
   "outputs": [],
   "source": [
    "p_estimations = pd.DataFrame(columns=['p' ,'method', 'mean_p', 'std_p'])\n",
    "scores = pd.DataFrame(columns=['p' ,'method', 'mean_f1', 'std_f1', 'mean_recall', 'std_recall',\n",
    "                               'mean_precision', 'std_precision', 'mean_fpr', 'std_fpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:11:42.088739Z",
     "start_time": "2018-06-18T12:33:16.940040Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:20<06:25, 20.28s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:27<04:03, 13.52s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:38<03:36, 12.75s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:45<03:02, 11.42s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:51<02:34, 10.33s/it]\u001b[A\n",
      " 30%|███       | 6/20 [02:20<05:27, 23.38s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [02:49<05:15, 24.27s/it]\u001b[A\n",
      " 40%|████      | 8/20 [03:11<04:47, 23.96s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [06:21<07:45, 42.34s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [06:27<06:27, 38.73s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [06:46<05:32, 36.95s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [06:51<04:34, 34.30s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [07:00<03:46, 32.31s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [07:07<03:03, 30.54s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [07:58<02:39, 31.90s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [08:08<02:02, 30.51s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [08:14<01:27, 29.07s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [08:19<00:55, 27.72s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [08:25<00:26, 26.61s/it]\u001b[A\n",
      "100%|██████████| 20/20 [09:23<00:00, 28.18s/it]\u001b[A\n",
      " 10%|█         | 1/10 [09:23<1:24:33, 563.68s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:08<02:41,  8.51s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:17<02:38,  8.81s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:35<03:19, 11.71s/it]\u001b[A\n",
      " 20%|██        | 4/20 [02:22<09:29, 35.62s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [02:31<07:35, 30.37s/it]\u001b[A\n",
      " 30%|███       | 6/20 [02:43<06:21, 27.25s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [03:39<06:47, 31.33s/it]\u001b[A\n",
      " 40%|████      | 8/20 [03:52<05:48, 29.03s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [04:20<05:18, 28.95s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [04:29<04:29, 26.99s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [04:47<03:55, 26.16s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [05:41<03:47, 28.44s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [06:14<03:21, 28.84s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [06:38<02:50, 28.44s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [06:48<02:16, 27.24s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [07:01<01:45, 26.33s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [07:16<01:16, 25.66s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [07:27<00:49, 24.87s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [07:45<00:24, 24.51s/it]\u001b[A\n",
      "100%|██████████| 20/20 [08:11<00:00, 24.57s/it]\u001b[A\n",
      " 20%|██        | 2/10 [17:35<1:10:20, 527.57s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:41<13:17, 41.99s/it]\u001b[A\n",
      " 10%|█         | 2/20 [01:02<09:18, 31.02s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [02:15<12:48, 45.23s/it]\u001b[A\n",
      " 20%|██        | 4/20 [04:04<16:19, 61.20s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [04:23<13:11, 52.76s/it]\u001b[A\n",
      " 30%|███       | 6/20 [04:38<10:50, 46.46s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [06:26<11:57, 55.21s/it]\u001b[A\n",
      " 40%|████      | 8/20 [06:38<09:58, 49.85s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [06:56<08:28, 46.22s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [07:16<07:16, 43.68s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [07:41<06:17, 41.92s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [08:04<05:23, 40.41s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [08:37<04:38, 39.81s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [09:14<03:57, 39.60s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [10:50<03:36, 43.35s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [11:06<02:46, 41.68s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [11:27<02:01, 40.44s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [11:50<01:18, 39.47s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [12:11<00:38, 38.48s/it]\u001b[A\n",
      "100%|██████████| 20/20 [12:42<00:00, 38.12s/it]\u001b[A\n",
      " 30%|███       | 3/10 [30:17<1:10:40, 605.85s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "for p in trange(2, max_p, 2):\n",
    "    incoming_edges = 3\n",
    "    f1 = {ic: [] for ic in ics}\n",
    "    f1.update({'incremental_' + ic: [] for ic in ics})\n",
    "    f1['real'] = []\n",
    "    \n",
    "    precision = {ic: [] for ic in ics}\n",
    "    precision.update({'incremental_' + ic: [] for ic in ics})\n",
    "    precision['real'] = []\n",
    "    \n",
    "    recall = {ic: [] for ic in ics}\n",
    "    recall.update({'incremental_' + ic: [] for ic in ics})\n",
    "    recall['real'] = []\n",
    "    \n",
    "    fpr = {ic: [] for ic in ics}\n",
    "    fpr.update({'incremental_' + ic: [] for ic in ics})\n",
    "    fpr['real'] = []\n",
    "    \n",
    "    p_est = {ic: [] for ic in ics}\n",
    "    p_est.update({'incremental_' + ic: [] for ic in ics})\n",
    "    \n",
    "    for i in trange(runs):\n",
    "        generator = CausalTSGenerator(dimensions=dimensions, max_p=p, data_length=data_length,\n",
    "                                      incoming_edges=incoming_edges)\n",
    "        ts = generator.generate()\n",
    "        predicted_graph = pc_chen_modified(partial_corr_test, ts, p, alpha)\n",
    "        \n",
    "        eval_result = evaluate_edges(generator.graph, predicted_graph)\n",
    "        f1['real'].append(eval_result['f1-score'])\n",
    "        recall['real'].append(eval_result['TPR'])\n",
    "        fpr['real'].append(eval_result['FPR'])\n",
    "        precision['real'].append(eval_result['precision'])\n",
    "        \n",
    "        # VAR estimation\n",
    "        var_ranking, var_scores = var_order_select(ts, 2*(max_p-2), ics, drop=True) \n",
    "        for ic in ics:\n",
    "            predicted_graph = pc_chen_modified(partial_corr_test, ts, var_ranking[ic][0], alpha)\n",
    "            eval_result = evaluate_edges(generator.graph, predicted_graph)\n",
    "            \n",
    "            p_est[ic].append(var_ranking[ic][0])\n",
    "            f1[ic].append(eval_result['f1-score'])\n",
    "            recall[ic].append(eval_result['TPR'])\n",
    "            fpr[ic].append(eval_result['FPR'])\n",
    "            precision[ic].append(eval_result['precision'])\n",
    "        \n",
    "        # incremental\n",
    "        for ic in ics:\n",
    "            predicted_graph = pc_incremental(partial_corr_test, ts, 0.05, 2*(max_p-2), ic=ic)\n",
    "            eval_result = evaluate_edges(generator.graph, predicted_graph)\n",
    "            \n",
    "            p_est['incremental_' + ic].append(len(predicted_graph.nodes())/dimensions - 1)\n",
    "            f1['incremental_' + ic].append(eval_result['f1-score'])\n",
    "            recall['incremental_' + ic].append(eval_result['TPR'])\n",
    "            fpr['incremental_' + ic].append(eval_result['FPR'])\n",
    "            precision['incremental_' + ic].append(eval_result['precision'])\n",
    "    \n",
    "    \n",
    "    scores = scores.append({'p': p, 'method': 'real',\n",
    "                            'mean_f1': np.mean(f1['real']), 'std_f1': np.std(f1['real']),\n",
    "                            'mean_recall': np.mean(recall['real']), 'std_recall': np.std(recall['real']),\n",
    "                            'mean_precision': np.mean(precision['real']), 'std_precision': np.std(precision['real']),\n",
    "                            'mean_fpr': np.mean(fpr['real']), 'std_fpr': np.std(fpr['real'])},\n",
    "                           ignore_index=True)\n",
    "    for ic in ics:\n",
    "        p_estimations = p_estimations.append({'p': p, 'method': ic, 'mean_p': np.mean(p_est[ic]), \n",
    "                                              'std_p': np.std(p_est[ic])},\n",
    "                                             ignore_index=True)\n",
    "        scores = scores.append({'p': p, 'method': ic,\n",
    "                                'mean_f1': np.mean(f1[ic]), 'std_f1': np.std(f1[ic]),\n",
    "                                'mean_recall': np.mean(recall[ic]), 'std_recall': np.std(recall[ic]),\n",
    "                                'mean_precision': np.mean(precision[ic]), 'std_precision': np.std(precision[ic]),\n",
    "                                'mean_fpr': np.mean(fpr[ic]), 'std_fpr': np.std(fpr[ic])},\n",
    "                               ignore_index=True)\n",
    "   \n",
    "    for ic in ics:\n",
    "        p_estimations = p_estimations.append({'p': p, 'method': 'incremental_' + ic, \n",
    "                                              'mean_p': np.mean(p_est['incremental_' + ic]),\n",
    "                                              'std_p': np.std(p_est['incremental_' + ic])},\n",
    "                                             ignore_index=True)\n",
    "        scores = scores.append({'p': p, 'method': 'incremental_' + ic,\n",
    "                                'mean_f1': np.mean(f1['incremental_' + ic]), 'std_f1': np.std(f1['incremental_' + ic]),\n",
    "                                'mean_recall': np.mean(recall['incremental_' + ic]),\n",
    "                                'std_recall': np.std(recall['incremental_' + ic]),\n",
    "                                'mean_precision': np.mean(precision['incremental_' + ic]),\n",
    "                                'std_precision': np.std(precision['incremental_' + ic]),\n",
    "                                'mean_fpr': np.mean(fpr['incremental_' + ic]),\n",
    "                                'std_fpr': np.std(fpr['incremental_' + ic])},\n",
    "                               ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:11:42.088739Z",
     "start_time": "2018-06-18T12:33:16.940040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.to_csv('scores_drop.csv')\n",
    "p_estimations.to_csv('p_estimations_drop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('f1 scores var estimation')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('f1')\n",
    "\n",
    "for i in range(len(ics)):\n",
    "    plt.subplot(len(ics), 1, i+1)\n",
    "    ic = ics[i]\n",
    "    plt.errorbar(x=scores.loc[scores['method']==ic, 'p'],\n",
    "                 y=scores.loc[scores['method']==ic, 'mean_f1'],\n",
    "                 yerr=scores.loc[scores['method']==ic, 'std_f1'],\n",
    "                 label=ic)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('f1 scores incremental')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('f1')\n",
    "\n",
    "for i in range(len(ics)):\n",
    "    plt.subplot(len(ics), 1, i+1)\n",
    "    ic = ics[i]\n",
    "    plt.errorbar(x=scores.loc[scores['method']=='incremental_' + ic, 'p'],\n",
    "                 y=scores.loc[scores['method']=='incremental_' + ic, 'mean_f1'],\n",
    "                 yerr=scores.loc[scores['method']=='incremental_' + ic, 'std_f1'],\n",
    "                 label='incremental_' + ic)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('f1 scores compared')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('f1')\n",
    "\n",
    "plt.errorbar(x=scores.loc[scores['method']=='real', 'p'],\n",
    "             y=scores.loc[scores['method']=='real', 'mean_f1'],\n",
    "             yerr=scores.loc[scores['method']=='real', 'std_f1'],\n",
    "             label='real')\n",
    "\n",
    "plt.errorbar(x=scores.loc[scores['method']=='bic', 'p'],\n",
    "             y=scores.loc[scores['method']=='bic', 'mean_f1'],\n",
    "             yerr=scores.loc[scores['method']=='bic', 'std_f1'],\n",
    "             label='bic')\n",
    "\n",
    "plt.errorbar(x=scores.loc[scores['method']=='incremental_bic', 'p'],\n",
    "             y=scores.loc[scores['method']=='incremental_bic', 'mean_f1'],\n",
    "             yerr=scores.loc[scores['method']=='incremental_bic', 'std_f1'],\n",
    "             label='incremental_bic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
